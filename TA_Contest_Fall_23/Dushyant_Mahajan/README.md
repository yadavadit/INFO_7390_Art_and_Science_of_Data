# Week 8: Using Generative AI for Synthetic Data Creation: Text

Welcome to Week 8 of INFO 7390!
This week we will be looking at Generative AI. We will be focusing on the generation of text using the renowned generative AI architecture of Generative Pre-trained Transformers. Follow along the course content with the provided notebook link below.
The video lesson has been uploaded to **[Youtube](https://www.youtube.com/watch?v=ZqRLpIuGMhY)**

## Table  of Contents
### 1. Introduction to Generative AI
- Unveiling the Fascinating World of Generative Pre-Trained Transformers
- Understanding GPT's Role in Text Generation
- GPT: The Tech Wizardry Transforming Text
### 2. Language Models and GPT's Text Generation
- Language Models: Unveiling the Magic
- GPT's Predictive Power: Next Word Prediction
- GPT's Abilities: Content Writing, Chatbots, and More
### 3. Mechanisms of Contextual Understanding
- Contextual Mastery: GPT's Superpower
- Causal Self-Attention: Maintaining Coherence
- Flowing Conversations: Seamless Context Generation
### 4. Attention Mechanism and Nuanced Understanding
- Spotlight on Understanding: Attention Mechanism
- Context Unleashed: Attention Heads' Selective Focus
- Complex Nuances: Multi-Head Attention's Role
### 5. Building GPT Language Models
- From Raw Data to Intelligence: GPT's Training
- Pre-training: Learning from Vast Text Corpora
- Fine-Tuning: Tailoring GPT to Specific Tasks
### 6. Conclusion
- A summary of the covered topics and concepts.

## Resources

1. Youtube Lesson Link: https://www.youtube.com/watch?v=ZqRLpIuGMhY
2. Presentation Link: https://docs.google.com/presentation/d/1X2ZoMhAUlUMI3HqbuYsOnV0TDb3c8EzBe98-jLwq-Hs/edit?usp=sharing
3. Notebook Link: https://colab.research.google.com/drive/1zhm3zs_-6QpcbOFT26yMWQo9nJd-nVD1?usp=sharing

## Additional Resources and References 

1. https://arxiv.org/abs/1706.03762 
2. https://openai.com/blog/chatgpt 
3. https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html
4. https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021
5. https://hackernoon.com/why-is-gpt-better-than-bert-a-detailed-review-of-transformer-architectures
6. https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4055s


Questions? Reach me at mahajan.d@northeastern.edu